{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b933de",
   "metadata": {},
   "source": [
    "# United Nations Parallel Corpora Analysis\n",
    "#### Kinan Al-Mouk  - kim47@pitt.edu - March 16th 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc58e9",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "[Uploading and Analyzing Raw Data](#Uploading-and-Analyzing-Raw-Data)\n",
    "\n",
    "- [English](#English)\n",
    "\n",
    "- [Spanish](#Spanish)\n",
    "\n",
    "- [French](#French)\n",
    "\n",
    "- [Russian](#Russian)\n",
    "\n",
    "- [Arabic](#Arabic)\n",
    "\n",
    "- [Mandarin](#Mandarin)\n",
    "\n",
    "[Creating Initial DataFrame for Analysis](#DataFrame-Construction)\n",
    "\n",
    "- [Analysis](#Sixway-Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7687c17d",
   "metadata": {},
   "source": [
    "# Uploading and Analyzing Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1ae879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import pickle\n",
    "from time import time\n",
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50101cfc",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6575d513",
   "metadata": {},
   "source": [
    "### Reading in English File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d62b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in: 0.20158886909484863 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "f = open('data/sixway/english.100k', 'r') # Reading in English File\n",
    "english100 = f.read()\n",
    "print(\"Data loaded in:\", (time()-start), \"seconds.\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb73d9e7",
   "metadata": {},
   "source": [
    "### Word Tokenizing English File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "933a9347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English word tokenized in: 18.570607900619507 seconds.\n",
      "Word Token Count for English File: 3105868\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenization\n",
    "start = time()\n",
    "small_en_words = nltk.word_tokenize(english100)\n",
    "print(\"English word tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_en_words_len = len(small_en_words)\n",
    "print(\"Word Token Count for English File:\", small_en_words_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2de4e2",
   "metadata": {},
   "source": [
    "### Sentence Tokenizing English File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bdd49a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentence tokenized in: 5.034231901168823 seconds.\n",
      "Sentence Token Count for English File: 108375\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "start = time()\n",
    "small_en_sents = nltk.sent_tokenize(english100)\n",
    "print(\"English sentence tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_en_sents_len = len(small_en_sents)\n",
    "print(\"Sentence Token Count for English File:\", small_en_sents_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47870855",
   "metadata": {},
   "source": [
    "# Spanish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac689b4",
   "metadata": {},
   "source": [
    "### Reading in Spanish File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "069f7ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in: 0.18303704261779785 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "f = open('data/sixway/spanish.100k', 'r') # Reading in Spanish File\n",
    "spanish100 = f.read()\n",
    "print(\"Data loaded in:\", (time()-start), \"seconds.\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50c1532",
   "metadata": {},
   "source": [
    "### Word Tokenizing Spanish File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dccd47a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish word tokenized in: 20.98197078704834 seconds.\n",
      "Word Token Count for Spanish File: 3504309\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "small_es_words = nltk.word_tokenize(spanish100)\n",
    "print(\"Spanish word tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_es_words_len = len(small_es_words)\n",
    "print(\"Word Token Count for Spanish File:\", small_es_words_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eb9410",
   "metadata": {},
   "source": [
    "### Sentence Tokenizing Spanish File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "433ea6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish sentence tokenized in: 5.2910449504852295 seconds.\n",
      "Sentence Token Count for English File: 102386\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "start = time()\n",
    "small_es_sents = nltk.sent_tokenize(spanish100)\n",
    "print(\"Spanish sentence tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_es_sents_len = len(small_es_sents)\n",
    "print(\"Sentence Token Count for English File:\", small_es_sents_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1e4ccb",
   "metadata": {},
   "source": [
    "# French"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304fab5d",
   "metadata": {},
   "source": [
    "### Reading in French File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e377ba29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in: 0.25301671028137207 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "f = open('data/sixway/french.100k', 'r') # Reading in French File\n",
    "french100 = f.read()\n",
    "print(\"Data loaded in:\", (time()-start), \"seconds.\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1142a6fc",
   "metadata": {},
   "source": [
    "### Word Tokenizing French File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "258e5839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French word tokenized in: 23.894285202026367 seconds.\n",
      "Word Token Count for French File: 3456688\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "small_fr_words = nltk.word_tokenize(french100)\n",
    "print(\"French word tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_fr_words_len = len(small_fr_words)\n",
    "print(\"Word Token Count for French File:\", small_fr_words_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ca46f8",
   "metadata": {},
   "source": [
    "### Sentence Tokenizing French File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e33be35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French sentence tokenized in: 6.414879083633423 seconds.\n",
      "Sentence Token Count for French File: 107730\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "start = time()\n",
    "small_fr_sents = nltk.sent_tokenize(french100)\n",
    "print(\"French sentence tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_fr_sents_len = len(small_fr_sents)\n",
    "print(\"Sentence Token Count for French File:\", small_fr_sents_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6445a0b9",
   "metadata": {},
   "source": [
    "# Russian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd20723",
   "metadata": {},
   "source": [
    "### Reading in Russian File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a744f798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in: 0.1800708770751953 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "f = open('data/sixway/russian.100k', 'r') # Reading in Russian File\n",
    "russian100 = f.read()\n",
    "print(\"Data loaded in:\", (time()-start), \"seconds.\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af737e74",
   "metadata": {},
   "source": [
    "### Word Tokenizing Russian File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f80a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "small_ru_words = nltk.word_tokenize(russian100)\n",
    "print(\"Russian word tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_ru_words_len = len(small_ru_words)\n",
    "print(\"Word Token Count for Russian File:\", small_ru_words_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bbf0a4",
   "metadata": {},
   "source": [
    "### Sentence Tokenizing Russian File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b761fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Tokenization\n",
    "start = time()\n",
    "small_ru_sents = nltk.sent_tokenize(russian100)\n",
    "print(\"Russian sentence tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_ru_sents_len = len(small_ru_sents)\n",
    "print(\"Sentence Token Count for Russian File:\", small_ru_sents_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0defdf51",
   "metadata": {},
   "source": [
    "# Arabic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b729b43",
   "metadata": {},
   "source": [
    "### Reading in Arabic File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33600ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "f = open('data/sixway/arabic.100k', 'r') # Reading in Arabic File\n",
    "arabic100 = f.read()\n",
    "print(\"Data loaded in:\", (time()-start), \"seconds.\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4e0310",
   "metadata": {},
   "source": [
    "### Word Tokenizing Arabic File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3412deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "small_ar_words = nltk.word_tokenize(arabic100)\n",
    "print(\"Arabic word tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_ar_words_len = len(small_ar_words)\n",
    "print(\"Word Token Count for Arabic File:\", small_ar_words_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe5e6ab",
   "metadata": {},
   "source": [
    "### Sentence Tokenizing Arabic File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d211cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Tokenization\n",
    "start = time()\n",
    "small_ar_sents = nltk.sent_tokenize(arabic100)\n",
    "print(\"Arabic sentence tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_ar_sents_len = len(small_ar_sents)\n",
    "print(\"Sentence Token Count for Arabic File:\", small_ar_sents_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3372e73",
   "metadata": {},
   "source": [
    "# Mandarin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b6a33",
   "metadata": {},
   "source": [
    "### Reading in Mandarin File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6914247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "f = open('data/sixway/mandarin.100k', 'r') # Reading in Mandarin File\n",
    "mandarin100 = f.read()\n",
    "print(\"Data loaded in:\", (time()-start), \"seconds.\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43c6239",
   "metadata": {},
   "source": [
    "### Word Tokenizing Mandarin File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1791f0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "small_zh_words = nltk.word_tokenize(mandarin100)\n",
    "print(\"Mandarin word tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_zh_words_len = len(small_zh_words)\n",
    "print(\"Word Token Count for Mandarin File:\", small_zh_words_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c109b9c5",
   "metadata": {},
   "source": [
    "### Sentence Tokenizing Mandarin File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cedb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Tokenization\n",
    "start = time()\n",
    "small_zh_sents = nltk.sent_tokenize(mandarin100)\n",
    "print(\"Mandarin sentence tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_zh_sents_len = len(small_zh_sents)\n",
    "print(\"Sentence Token Count for Mandarin File:\", small_zh_sents_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa8e077",
   "metadata": {},
   "source": [
    "# DataFrame Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be75f7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Language': ['English', 'Spanish', 'French', 'Russian', 'Arabic', 'Mandarin'], \n",
    "        'Text': [english100, spanish100, french100, russian100, arabic100, mandarin100],\n",
    "        'Word Tokens' : [small_en_words, small_es_words, small_fr_words, small_ru_words, small_ar_words, small_zh_words],\n",
    "        'Word Tokens Len' : [small_en_words_len, small_es_words_len, small_fr_words_len, small_ru_words_len, small_ar_words_len, small_zh_words_len],  \n",
    "        'Sentence Tokens' : [small_en_sents, small_es_sents, small_fr_sents, small_ru_sents, small_ar_sents, small_zh_sents],  \n",
    "        'Sentence Tokens Len' : [small_en_sents_len, small_es_sents_len, small_fr_sents_len, small_ru_sents_len, small_ar_sents_len, small_zh_sents_len] } \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9e4b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "sixway_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc57134",
   "metadata": {},
   "outputs": [],
   "source": [
    "sixway_df['Average Sentence Length'] = sixway_df['Word Tokens Len']/sixway_df['Sentence Tokens Len']\n",
    "sixway_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c5a385",
   "metadata": {},
   "outputs": [],
   "source": [
    "sixway_df.to_pickle(\"sixway_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867ae1b2",
   "metadata": {},
   "source": [
    "## Word Tokens Length Comparsion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ad2c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "sixway_df.plot.bar(x='Language', y='Word Tokens Len', rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4230cc05",
   "metadata": {},
   "source": [
    "## Sentence Tokens Length Comparsion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51807d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sixway_df.plot.bar(x='Language', y='Sentence Tokens Len', rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449d8213",
   "metadata": {},
   "source": [
    "## Average Sentence Length Comparsion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c505bb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sixway_df.plot.bar(x='Language', y='Average Sentence Length', rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f0d6e3",
   "metadata": {},
   "source": [
    "## Sixway Analysis \n",
    " - **Mandarin** has the least amount of Word Tokens at **375,501** where as **English, Spanish, and French** have over **3 million**. Not sure what the reason for this is at the moment. Same phenmomena occurs for Sentence Tokens Length. \n",
    " - **Mandarin** Has the lowest counts for Word Tokens Length, Sentence Tokens Length, and Average Sentence Length."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

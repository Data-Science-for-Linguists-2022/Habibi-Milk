{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b933de",
   "metadata": {},
   "source": [
    "# United Nations Parallel Corpora Analysis\n",
    "#### Kinan Al-Mouk  - kim47@pitt.edu - February 17th 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc58e9",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "  ## Language Files\n",
    "  **Spanish**\n",
    "\n",
    "  **Russian**\n",
    "  \n",
    "  **Arabic**\n",
    "  \n",
    "  **English**\n",
    "  \n",
    "  **Mandarin**\n",
    "  \n",
    "  **French**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1ae879c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kinanmouk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')\n",
    "from time import time\n",
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50101cfc",
   "metadata": {},
   "source": [
    "# Spanish "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6575d513",
   "metadata": {},
   "source": [
    "### Reading in Spanish File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d62b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in: 130.7019920349121 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "f = open('UNv1.0.6way.es', 'r') # Reading in Spanish File\n",
    "es = f.read()\n",
    "print(\"Data loaded in:\", (time()-start), \"seconds.\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4985f2a2",
   "metadata": {},
   "source": [
    "### Creating Workable Spanish File Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "930711ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portioning File into a smaller slice so that it is easier to work with for now\n",
    "small_es = es[:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb73d9e7",
   "metadata": {},
   "source": [
    "### Word Tokenizing Small Spanish File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "933a9347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish word tokenized in: 0.3961679935455322 seconds.\n",
      "Word Token Count for Spanish Files: 17218\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenization\n",
    "start = time()\n",
    "small_es_words = nltk.word_tokenize(small_es)\n",
    "print(\"Spanish word tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_es_words_len = len(small_es_words)\n",
    "print(\"Word Token Count for Spanish Files:\", small_es_words_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2de4e2",
   "metadata": {},
   "source": [
    "### Sentence Tokenizing Small Spanish File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bdd49a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish sentence tokenized in: 0.06349396705627441 seconds.\n",
      "Sentence Token Count for Spanish Files: 407\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "start = time()\n",
    "small_es_sents = nltk.sent_tokenize(small_es)\n",
    "print(\"Spanish sentence tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_es_sents_len = len(small_es_sents)\n",
    "print(\"Sentence Token Count for Spanish Files:\", small_es_sents_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6452cdff",
   "metadata": {},
   "source": [
    "# Russian "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dcd9f6",
   "metadata": {},
   "source": [
    "### Reading in Russian File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f2a533a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in: 167.14568305015564 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "f = open('UNv1.0.6way.ru', 'r', encoding = 'utf-8') # Reading in Russian File\n",
    "ru = f.read()\n",
    "print(\"Data loaded in:\", (time()-start), \"seconds.\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b01b8a",
   "metadata": {},
   "source": [
    "### Creating Workable Russian File Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eb6ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ru = ru[:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accda2cb",
   "metadata": {},
   "source": [
    "### Word Tokenizing Small Russian File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6630df1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian word tokenized in: 0.21197724342346191 seconds.\n",
      "Russian Word Token Count for Russian Files: 14789\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenization\n",
    "start = time()\n",
    "small_ru_words = nltk.word_tokenize(small_ru)\n",
    "print(\"Russian word tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_ru_words_len = len(small_ru_words)\n",
    "print(\"Russian Word Token Count for Russian Files:\", small_ru_words_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74cf9a4",
   "metadata": {},
   "source": [
    "### Sentence Tokenizing Small Russian File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfdfb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Tokenization\n",
    "start = time()\n",
    "small_ru_sents = nltk.sent_tokenize(small_ru)\n",
    "print(\"Russian sentence tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_ru_sents_len = len(small_ru_sents)\n",
    "print(\"Russian Token Count for Russian Files:\", small_ru_sents_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceb7ee8",
   "metadata": {},
   "source": [
    "# Arabic "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deab6818",
   "metadata": {},
   "source": [
    "### Reading in Arabic File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17f101b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in: 108.79556226730347 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "f = open('UNv1.0.6way.ar', 'r', encoding = 'utf-8') # Reading in Arabic File\n",
    "ar = f.read()\n",
    "print(\"Data loaded in:\", (time()-start), \"seconds.\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69066318",
   "metadata": {},
   "source": [
    "### Creating Workable Arabic File Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fe93043",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ar = ar[:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cb4c6f",
   "metadata": {},
   "source": [
    "### Word Tokenizing Small Arabic File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87b6de65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic word tokenized in: 0.20563817024230957 seconds.\n",
      "Arabic Word Token Count for Arabic Files: 17783\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenization\n",
    "start = time()\n",
    "small_ar_words = nltk.word_tokenize(small_ar)\n",
    "print(\"Arabic word tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_ar_words_len = len(small_ar_words)\n",
    "print(\"Arabic Word Token Count for Arabic Files:\", small_ar_words_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb4d874",
   "metadata": {},
   "source": [
    "### Sentence Tokenizing Small Arabic File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77af26a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic sentence tokenized in: 0.0327601432800293 seconds.\n",
      "Arabic Sentence Token Count for Arabic Files: 526\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "start = time()\n",
    "small_ar_sents = nltk.sent_tokenize(small_ar)\n",
    "print(\"Arabic sentence tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_ar_sents_len = len(small_ar_sents)\n",
    "print(\"Arabic Sentence Token Count for Arabic Files:\", small_ar_sents_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d0ac4",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7162ab",
   "metadata": {},
   "source": [
    "### Reading in English File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26044f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "f = open('UNv1.0.6way.en', 'r', encoding = 'utf-8') # Reading in English File\n",
    "en = f.read()\n",
    "print(\"Data loaded in:\", (time()-start), \"seconds.\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42a09a9",
   "metadata": {},
   "source": [
    "### Creating Workable English File Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaea46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_en = en[:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c66f6f",
   "metadata": {},
   "source": [
    "### Word Tokenizing Small English File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc86bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Tokenization\n",
    "start = time()\n",
    "small_en_words = nltk.word_tokenize(small_en)\n",
    "print(\"English word tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_en_words_len = len(small_en_words)\n",
    "print(\"English Word Token Count for English Files:\", small_en_words_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cea9a6",
   "metadata": {},
   "source": [
    "### Sentence Tokenizing Small English File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c7f89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Tokenization\n",
    "start = time()\n",
    "small_en_sents = nltk.sent_tokenize(small_en)\n",
    "print(\"English sentence tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_en_sents_len = len(small_en_sents)\n",
    "print(\"English Sentence Token Count for English Files:\", small_en_sents_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0139dde",
   "metadata": {},
   "source": [
    "# Mandarin "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca937c9d",
   "metadata": {},
   "source": [
    "### Reading in Mandarin File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8581854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "f = open('UNv1.0.6way.zh', 'r', encoding = 'utf-8') # Reading in Mandarin File\n",
    "zh = f.read()\n",
    "print(\"Data loaded in:\", (time()-start), \"seconds.\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5494343",
   "metadata": {},
   "source": [
    "### Creating Workable Mandarin File Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c3ebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_zh = zh[:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3e0b15",
   "metadata": {},
   "source": [
    "### Word Tokenizing Small Mandarin File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd213f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Tokenization\n",
    "start = time()\n",
    "small_zh_words = nltk.word_tokenize(small_zh)\n",
    "print(\"Mandarin word tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_zh_words_len = len(small_zh_words)\n",
    "print(\"Mandarin Word Token Count for Mandarin Files:\", small_zh_words_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e66cd5b",
   "metadata": {},
   "source": [
    "### Sentence Tokenizing Small Mandarin File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdec3505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Tokenization\n",
    "start = time()\n",
    "small_zh_sents = nltk.sent_tokenize(small_zh)\n",
    "print(\"Mandarin sentence tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_zh_sents_len = len(small_zh_sents)\n",
    "print(\"Mandarin Sentence Token Count for Mandarin Files:\", small_zh_sents_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89c936e",
   "metadata": {},
   "source": [
    "# French "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62fe86c",
   "metadata": {},
   "source": [
    "### Reading in French File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cc4e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "f = open('UNv1.0.6way.fr', 'r', encoding = 'utf-8') # Reading in French File\n",
    "fr = f.read()\n",
    "print(\"Data loaded in:\", (time()-start), \"seconds.\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d56f7f",
   "metadata": {},
   "source": [
    "### Creating Workable French File Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e32ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_fr = fr[:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b03d0dc",
   "metadata": {},
   "source": [
    "### Word Tokenizing Small French File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977cf163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Tokenization\n",
    "start = time()\n",
    "small_fr_words = nltk.word_tokenize(small_fr)\n",
    "print(\"French word tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_fr_words_len = len(small_fr_words)\n",
    "print(\"French Word Token Count for Mandarin Files:\", small_fr_words_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c299df2",
   "metadata": {},
   "source": [
    "### Sentence Tokenizing Small French File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b557f566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Tokenization\n",
    "start = time()\n",
    "small_fr_sents = nltk.sent_tokenize(small_fr)\n",
    "print(\"French sentence tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_fr_sents_len = len(small_fr_sents)\n",
    "print(\"French Sentence Token Count for French Files:\", small_fr_sents_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b933de",
   "metadata": {},
   "source": [
    "# United Nations Parallel Corpora Analysis\n",
    "#### Kinan Al-Mouk  - kim47@pitt.edu - March 16th 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc58e9",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "[Uploading and Analyzing Raw Data](#Uploading-and-Analyzing-Raw-Data)\n",
    "\n",
    "- [English](#English)\n",
    "\n",
    "- [Spanish](#Spanish)\n",
    "\n",
    "- [French](#French)\n",
    "\n",
    "- [Russian](#Russian)\n",
    "\n",
    "- [Arabic](#Arabic)\n",
    "\n",
    "- [Mandarin](#Mandarin)\n",
    "\n",
    "[Creating Initial DataFrame for Analysis](#DataFrame-Construction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02fa078",
   "metadata": {},
   "source": [
    "# Uploading and Analyzing Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1ae879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from time import time\n",
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50101cfc",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6575d513",
   "metadata": {},
   "source": [
    "### Reading in English File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d62b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in: 0.15114116668701172 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "f = open('data/sixway/english.100k', 'r') # Reading in English File\n",
    "english100 = f.read()\n",
    "print(\"Data loaded in:\", (time()-start), \"seconds.\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb73d9e7",
   "metadata": {},
   "source": [
    "### Word Tokenizing English File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "933a9347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English word tokenized in: 20.7223219871521 seconds.\n",
      "Word Token Count for English File: 3105868\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenization\n",
    "start = time()\n",
    "small_en_words = nltk.word_tokenize(english100)\n",
    "print(\"English word tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_en_words_len = len(small_en_words)\n",
    "print(\"Word Token Count for English File:\", small_en_words_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2de4e2",
   "metadata": {},
   "source": [
    "### Sentence Tokenizing English File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bdd49a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentence tokenized in: 5.488654136657715 seconds.\n",
      "Sentence Token Count for English File: 108375\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "start = time()\n",
    "small_en_sents = nltk.sent_tokenize(english100)\n",
    "print(\"English sentence tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_en_sents_len = len(small_en_sents)\n",
    "print(\"Sentence Token Count for English File:\", small_en_sents_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb6aae",
   "metadata": {},
   "source": [
    "# Spanish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a423cd5",
   "metadata": {},
   "source": [
    "### Reading in Spanish File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "622d30eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in: 0.24316692352294922 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "f = open('data/sixway/spanish.100k', 'r') # Reading in Spanish File\n",
    "spanish100 = f.read()\n",
    "print(\"Data loaded in:\", (time()-start), \"seconds.\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9095517c",
   "metadata": {},
   "source": [
    "### Word Tokenizing Spanish File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "362b547b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish word tokenized in: 24.900392055511475 seconds.\n",
      "Word Token Count for Spanish File: 3504309\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "small_es_words = nltk.word_tokenize(spanish100)\n",
    "print(\"Spanish word tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_es_words_len = len(small_es_words)\n",
    "print(\"Word Token Count for Spanish File:\", small_es_words_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885f57a2",
   "metadata": {},
   "source": [
    "### Sentence Tokenizing Spanish File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb0caf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish sentence tokenized in: 6.110598802566528 seconds.\n",
      "Sentence Token Count for English File: 102386\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "start = time()\n",
    "small_es_sents = nltk.sent_tokenize(spanish100)\n",
    "print(\"Spanish sentence tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_es_sents_len = len(small_es_sents)\n",
    "print(\"Sentence Token Count for English File:\", small_es_sents_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff853a02",
   "metadata": {},
   "source": [
    "# French"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c655c1b",
   "metadata": {},
   "source": [
    "### Reading in French File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4044ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in: 0.25275087356567383 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "f = open('data/sixway/french.100k', 'r') # Reading in French File\n",
    "french100 = f.read()\n",
    "print(\"Data loaded in:\", (time()-start), \"seconds.\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ac4e81",
   "metadata": {},
   "source": [
    "### Word Tokenizing French File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c586034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French word tokenized in: 25.011726140975952 seconds.\n",
      "Word Token Count for French File: 3456688\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "small_fr_words = nltk.word_tokenize(french100)\n",
    "print(\"French word tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_fr_words_len = len(small_fr_words)\n",
    "print(\"Word Token Count for French File:\", small_fr_words_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9907e7",
   "metadata": {},
   "source": [
    "### Sentence Tokenizing French File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20897bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French sentence tokenized in: 6.825227975845337 seconds.\n",
      "Sentence Token Count for French File: 107730\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "start = time()\n",
    "small_fr_sents = nltk.sent_tokenize(french100)\n",
    "print(\"French sentence tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_fr_sents_len = len(small_fr_sents)\n",
    "print(\"Sentence Token Count for French File:\", small_fr_sents_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61878ea9",
   "metadata": {},
   "source": [
    "# Russian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3477ee76",
   "metadata": {},
   "source": [
    "### Reading in Russian File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37df8aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in: 0.21496081352233887 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "f = open('data/sixway/russian.100k', 'r') # Reading in Russian File\n",
    "russian100 = f.read()\n",
    "print(\"Data loaded in:\", (time()-start), \"seconds.\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7514603d",
   "metadata": {},
   "source": [
    "### Word Tokenizing Russian File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6872a69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian word tokenized in: 26.897891759872437 seconds.\n",
      "Word Token Count for Russian File: 2857554\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "small_ru_words = nltk.word_tokenize(russian100)\n",
    "print(\"Russian word tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_ru_words_len = len(small_ru_words)\n",
    "print(\"Word Token Count for Russian File:\", small_ru_words_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed89bde",
   "metadata": {},
   "source": [
    "### Sentence Tokenizing Russian File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a054b6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian sentence tokenized in: 8.108860969543457 seconds.\n",
      "Sentence Token Count for Russian File: 108311\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "start = time()\n",
    "small_ru_sents = nltk.sent_tokenize(russian100)\n",
    "print(\"Russian sentence tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_ru_sents_len = len(small_ru_sents)\n",
    "print(\"Sentence Token Count for Russian File:\", small_ru_sents_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16abdac3",
   "metadata": {},
   "source": [
    "# Arabic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcf6a4f",
   "metadata": {},
   "source": [
    "### Reading in Arabic File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84ebb4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in: 0.16884827613830566 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "f = open('data/sixway/arabic.100k', 'r') # Reading in Arabic File\n",
    "arabic100 = f.read()\n",
    "print(\"Data loaded in:\", (time()-start), \"seconds.\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ffa85d",
   "metadata": {},
   "source": [
    "### Word Tokenizing Arabic File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52f885c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic word tokenized in: 20.76917004585266 seconds.\n",
      "Word Token Count for Arabic File: 2564054\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "small_ar_words = nltk.word_tokenize(arabic100)\n",
    "print(\"Arabic word tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_ar_words_len = len(small_ar_words)\n",
    "print(\"Word Token Count for Arabic File:\", small_ar_words_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228bd241",
   "metadata": {},
   "source": [
    "### Sentence Tokenizing Arabic File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b21b65bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic sentence tokenized in: 3.8507251739501953 seconds.\n",
      "Sentence Token Count for Arabic File: 78687\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "start = time()\n",
    "small_ar_sents = nltk.sent_tokenize(arabic100)\n",
    "print(\"Arabic sentence tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_ar_sents_len = len(small_ar_sents)\n",
    "print(\"Sentence Token Count for Arabic File:\", small_ar_sents_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd2dafd",
   "metadata": {},
   "source": [
    "# Mandarin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc53d8a",
   "metadata": {},
   "source": [
    "### Reading in Mandarin File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2a79d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in: 0.09038591384887695 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "f = open('data/sixway/mandarin.100k', 'r') # Reading in Mandarin File\n",
    "mandarin100 = f.read()\n",
    "print(\"Data loaded in:\", (time()-start), \"seconds.\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0345c924",
   "metadata": {},
   "source": [
    "### Word Tokenizing Mandarin File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3dc1dcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mandarin word tokenized in: 9.78513479232788 seconds.\n",
      "Word Token Count for Mandarin File: 375501\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "small_zh_words = nltk.word_tokenize(mandarin100)\n",
    "print(\"Mandarin word tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_zh_words_len = len(small_zh_words)\n",
    "print(\"Word Token Count for Mandarin File:\", small_zh_words_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9e6e5b",
   "metadata": {},
   "source": [
    "### Sentence Tokenizing Mandarin File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "286e6255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mandarin sentence tokenized in: 4.953838109970093 seconds.\n",
      "Sentence Token Count for Mandarin File: 27314\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "start = time()\n",
    "small_zh_sents = nltk.sent_tokenize(mandarin100)\n",
    "print(\"Mandarin sentence tokenized in:\", (time()-start), \"seconds.\")\n",
    "small_zh_sents_len = len(small_zh_sents)\n",
    "print(\"Sentence Token Count for Mandarin File:\", small_zh_sents_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1bf648",
   "metadata": {},
   "source": [
    "# DataFrame Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f961a3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Language': ['English', 'Spanish', 'French', 'Russian', 'Arabic', 'Mandarin'], \n",
    "        'Text': [english100, spanish100, french100, russian100, arabic100, mandarin100],\n",
    "        'Word Tokens' : [small_en_words, small_es_words, small_fr_words, small_ru_words, small_ar_words, small_zh_words],\n",
    "        'Word Tokens Len' : [small_en_words_len, small_es_words_len, small_fr_words_len, small_ru_words_len, small_ar_words_len, small_zh_words_len],  \n",
    "        'Sentence Tokens' : [small_en_sents, small_es_sents, small_fr_sents, small_ru_sents, small_ar_sents, small_zh_sents],  \n",
    "        'Sentence Tokens Len' : [small_en_sents_len, small_es_sents_len, small_fr_sents_len, small_ru_sents_len, small_ar_sents_len, small_zh_sents_len] } \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20f7a45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sixway_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "16a34805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Text</th>\n",
       "      <th>Word Tokens</th>\n",
       "      <th>Word Tokens Len</th>\n",
       "      <th>Sentence Tokens</th>\n",
       "      <th>Sentence Tokens Len</th>\n",
       "      <th>Average Sentence Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>RESOLUTION 918 (1994)\\nAdopted by the Security...</td>\n",
       "      <td>[RESOLUTION, 918, (, 1994, ), Adopted, by, the...</td>\n",
       "      <td>3105868</td>\n",
       "      <td>[RESOLUTION 918 (1994)\\nAdopted by the Securit...</td>\n",
       "      <td>108375</td>\n",
       "      <td>28.658528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>RESOLUCIÓN 918 (1994)\\nAprobada por el Consejo...</td>\n",
       "      <td>[RESOLUCIÓN, 918, (, 1994, ), Aprobada, por, e...</td>\n",
       "      <td>3504309</td>\n",
       "      <td>[RESOLUCIÓN 918 (1994)\\nAprobada por el Consej...</td>\n",
       "      <td>102386</td>\n",
       "      <td>34.226447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>French</td>\n",
       "      <td>RESOLUTION 918 (1994)\\nAdoptée par le Conseil ...</td>\n",
       "      <td>[RESOLUTION, 918, (, 1994, ), Adoptée, par, le...</td>\n",
       "      <td>3456688</td>\n",
       "      <td>[RESOLUTION 918 (1994)\\nAdoptée par le Conseil...</td>\n",
       "      <td>107730</td>\n",
       "      <td>32.086587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Russian</td>\n",
       "      <td>РЕЗОЛЮЦИЯ 918 (1994),\\nпринятая Советом Безопа...</td>\n",
       "      <td>[РЕЗОЛЮЦИЯ, 918, (, 1994, ), ,, принятая, Сове...</td>\n",
       "      <td>2857554</td>\n",
       "      <td>[РЕЗОЛЮЦИЯ 918 (1994),\\nпринятая Советом Безоп...</td>\n",
       "      <td>108311</td>\n",
       "      <td>26.382860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>القرار ٨١٩ )٤٩٩١(\\nالذي اتخذه مجلس اﻷمن في جلس...</td>\n",
       "      <td>[القرار, ٨١٩, ), ٤٩٩١, (, الذي, اتخذه, مجلس, ا...</td>\n",
       "      <td>2564054</td>\n",
       "      <td>[القرار ٨١٩ )٤٩٩١(\\nالذي اتخذه مجلس اﻷمن في جل...</td>\n",
       "      <td>78687</td>\n",
       "      <td>32.585484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mandarin</td>\n",
       "      <td>第918(1994)号决议\\n1994年5月17日安全理事会第3377次会议通过\\n安全理事...</td>\n",
       "      <td>[第918, (, 1994, ), 号决议, 1994年5月17日安全理事会第3377次会...</td>\n",
       "      <td>375501</td>\n",
       "      <td>[第918(1994)号决议\\n1994年5月17日安全理事会第3377次会议通过\\n安全理...</td>\n",
       "      <td>27314</td>\n",
       "      <td>13.747565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Language                                               Text  \\\n",
       "0   English  RESOLUTION 918 (1994)\\nAdopted by the Security...   \n",
       "1   Spanish  RESOLUCIÓN 918 (1994)\\nAprobada por el Consejo...   \n",
       "2    French  RESOLUTION 918 (1994)\\nAdoptée par le Conseil ...   \n",
       "3   Russian  РЕЗОЛЮЦИЯ 918 (1994),\\nпринятая Советом Безопа...   \n",
       "4    Arabic  القرار ٨١٩ )٤٩٩١(\\nالذي اتخذه مجلس اﻷمن في جلس...   \n",
       "5  Mandarin  第918(1994)号决议\\n1994年5月17日安全理事会第3377次会议通过\\n安全理事...   \n",
       "\n",
       "                                         Word Tokens  Word Tokens Len  \\\n",
       "0  [RESOLUTION, 918, (, 1994, ), Adopted, by, the...          3105868   \n",
       "1  [RESOLUCIÓN, 918, (, 1994, ), Aprobada, por, e...          3504309   \n",
       "2  [RESOLUTION, 918, (, 1994, ), Adoptée, par, le...          3456688   \n",
       "3  [РЕЗОЛЮЦИЯ, 918, (, 1994, ), ,, принятая, Сове...          2857554   \n",
       "4  [القرار, ٨١٩, ), ٤٩٩١, (, الذي, اتخذه, مجلس, ا...          2564054   \n",
       "5  [第918, (, 1994, ), 号决议, 1994年5月17日安全理事会第3377次会...           375501   \n",
       "\n",
       "                                     Sentence Tokens  Sentence Tokens Len  \\\n",
       "0  [RESOLUTION 918 (1994)\\nAdopted by the Securit...               108375   \n",
       "1  [RESOLUCIÓN 918 (1994)\\nAprobada por el Consej...               102386   \n",
       "2  [RESOLUTION 918 (1994)\\nAdoptée par le Conseil...               107730   \n",
       "3  [РЕЗОЛЮЦИЯ 918 (1994),\\nпринятая Советом Безоп...               108311   \n",
       "4  [القرار ٨١٩ )٤٩٩١(\\nالذي اتخذه مجلس اﻷمن في جل...                78687   \n",
       "5  [第918(1994)号决议\\n1994年5月17日安全理事会第3377次会议通过\\n安全理...                27314   \n",
       "\n",
       "   Average Sentence Length  \n",
       "0                28.658528  \n",
       "1                34.226447  \n",
       "2                32.086587  \n",
       "3                26.382860  \n",
       "4                32.585484  \n",
       "5                13.747565  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sixway_df['Average Sentence Length'] = sixway_df['Word Tokens Len']/sixway_df['Sentence Tokens Len']\n",
    "sixway_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9b5c60",
   "metadata": {},
   "source": [
    "## Sixway Analysis \n",
    " - **Mandarin** has the least amount of Word Tokens at **375,501** where as **English, Spanish, and French** have over **3 million**. Not sure what the reason for this is at the moment. Same phenmomena occurs for Sentence Tokens Length. \n",
    " - **Mandarin** Has the lowest counts for Word Tokens Length, Sentence Tokens Length, and Average Sentence Length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3d2a53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
